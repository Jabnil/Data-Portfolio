name: Production Data Pipeline CI Project 1

on:
  push:
    paths:
      - 'project1-ecommerce/**'
  pull_request:
    paths:
      - 'project1-ecommerce/**'
  workflow_dispatch: # Allows manual trigger from the GitHub Actions tab

jobs:
  quality-and-pipeline:
    runs-on: ubuntu-latest

    services:
      # We use a service container for Postgres to mimic a production DB environment
      postgres:
        image: postgres:15-alpine
        env:
          POSTGRES_USER: analyst
          POSTGRES_PASSWORD: password123
          POSTGRES_DB: ecommerce_sales
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    defaults:
          run:
            working-directory: ./project1-ecommerce
    steps:
      - name: ğŸ›’ Checkout Repository
        uses: actions/checkout@v4

      - name: ğŸ Set up Python
        uses: actions/checkout@v4
        with:
          python-version: "3.11"

      - name: âš¡ Cache Python Packages
        uses: actions/cache@v3
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('p1-requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: ğŸ› ï¸ Install Dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flake8
          pip install --no-cache-dir -r project1-ecommerce/p1-requirements.txt

      - name: ğŸ” Lint Code (PEP8 Check)
        run: |
          # Stop the build if there are Python syntax errors or undefined names
          flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
          # exit-zero treats all errors as warnings. 127 chars is the GitHub default.
          flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics

      #- name: ğŸ—ï¸ Build Docker Environment
        #run: docker compose build

      - name: ğŸš€ 1. Create and start containers
        run: |
          docker compose up -d
      
      - name: â³ 2. Waiting for PostgreSQL to be ready
        run: |
          until docker exec ecommerce_db pg_isready -U analyst -d ecommerce_sales; do sleep 2 done

      - name: ğŸ§¹ 3. Running data cleaning and ETL pipeline
        run: |    
          # 3.1. Initialize Database Schema
          docker exec -i ecommerce_db psql -U analyst -d ecommerce_sales < sql/schema_setup.sql
          
          # 3.2. Generate Mock Data
          docker exec ecommerce_analysis python scripts/mock_data_generator.py
          
          # 3.3. Clean Data
          docker exec ecommerce_analysis python scripts/data_cleaning.py

          # 3.4. Verify Cleaned Data Exists
          docker exec ecommerce_analysis ls data/processed/cleaned_sales_data.csv
          
          # 3.5. Load to SQL
          docker exec -e DB_HOST=db ecommerce_analysis python scripts/load_to_sql.py

      - name: ğŸ“Š 4. Verify SQL Analysis
        run: |
          # Run one of your analysis queries to ensure data is queryable
          docker exec -i ecommerce_db psql -U analyst -d ecommerce_sales -c "SELECT category, SUM(sales_amount) FROM sales_data GROUP BY 1;"

      - name: ğŸ›‘ 5. Shutdown Services
        if: always()
        run: docker compose down